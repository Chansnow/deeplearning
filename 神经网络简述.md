# 神经网络简述

##### 神经网络优点

对所有的问题都可以用同样的流程来解决

##### 神经网络学习的目的

以损失函数为基准，找出能使它的值达到最小的权重参数

##### 机器学习的终极目标

获得泛化能力防止过拟合（只对某个数据集过度拟合）

---

#### 损失函数

表示神经网络性能的恶劣程度的指标

常用的有**均方误差**和**交叉熵误差**

##### 均方误差

单个数据的损失函数: $E=\frac{1}{2}\sum_k(y_k-t_k)^2$

> $y_k$表示神经网络的输出， $t_k$为监督数据，k为数据的维数

所有数据的平均损失函数: $E=\frac{1}{2n}\sum_n\sum_k(y_{nk}-t_{nk})^2$

##### 交叉熵误差

单个数据的损失函数: $E=-\sum_kt_k\log{y_k}$

在具体编程时常使用 $E=-\sum_kt_k\log{(y_k+\delta)}$ 加上微小值作为保护，防止log0出现负无穷

> 实际上只计算对应正确解标签的输出的对数（ $t_k$使用one-hot独热编码表示），即交叉熵误差的值是有正确解标签对应的输出结果决定

所有数据的平均损失函数: $E=-\frac{1}{n}\sum_n\sum_kt_{nk}\log{y_{nk}}$

##### 为何需要设定损失函数

因为如果以识别精度为指标，则参数的导数在绝大多数地方都会变为0

> 识别精度对微小参数变化基本上不会有什么反应（改变微小参数，识别精度可能仍然保持不变），即便有反应它的值也是不连续地、突然地变化
---

#### 数值微分

##### 求导

$\frac{df(x)}{dx}=lim_{h->0}\frac{f(x+h)-f(x)}{h}$

> h不能取过小，否则会产生舍入误差（因省略小数的精细部分的数值（如小数点第8位以后的数值）而造成最终计算结果上的误差）

数值微分存在误差，为减小误差，可以使用中心差分

$\frac{df(x)}{dx}=lim_{h->0}\frac{f(x+h)-f(x-h)}{2h}$

##### 梯度

像 $(\frac{\partial{f}}{\partial{x_0}},\frac{\partial{f}}{\partial{x_1}})$ 这样的由函数全部变量的偏导数汇总而成的向量叫做梯度

梯度是一个矢量，表示某一函数在该点处的方向导数沿着该方向取得最大值，即函数在该点处沿着该方向变化最快

负梯度方向是梯度法中变量的更新方向

---

#### 学习算法的实现

神经网络存在合适的权重和偏置，调整权重和偏置以便拟合训练数据的过程称为学习

**步骤1（mini-batch）**

随机选择一部分数据，目标是减小mini-batch的损失函数的值

**步骤2（计算梯度）**

为减小损失函数的值，需要求出各个权重参数的梯度

**步骤3（更新参数）**

将权重沿梯度方向进行微小更新

**步骤4（重复）**

重复以上步骤



