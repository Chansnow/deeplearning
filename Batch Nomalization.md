
# Batch Normalization

为能顺利地进行学习，神经网络需要各层有适当的广度

BN的思路是调整各层的激活函数值分布使其拥有适当的广度

#### BN的优点

- 可以增大学习率使学习快速进行
- 不那么依赖初始值
- 抑制过拟合

#### 做法

**减均值除方差：远离饱和区**

向神经网络中插入对数据分布进行正规化的BN层，进行使数据分布的均值为0、方差为1的正规化

$$
\mu_B\leftarrow\frac{1}{m}\sum_{i=1}^mx_i
$$

$$
\delta_B^2\leftarrow\frac{1}{m}\sum_{i=1}^m(x_i-\mu_B)^2
$$

$$
\hat{x_i}\leftarrow \frac{x_i-\mu_B}{\sqrt{\delta_B^2+\varepsilon}}
$$

> $\varepsilon$为微小值，防止出现除以0的情况

先对每个batch的m个输入数据求均值$\mu_B$和方差$\delta_B^2$，然后对输入数据进行均值为0、方差为1的正规化

但这个处理对于在-1~1之间的梯度变化不大的激活函数，效果会变差

对sigmod来说，它在-1~1之间几乎呈线性，BN变换之后没有达到非线性变换的目的

对ReLU来说，会有一半置0

所以要使用缩放因子γ和移位因子β来进行一些转换将分布从0移开。

**缩放加移位：避免线性区**

$$
y_i\leftarrow\gamma\hat{x_i}+\beta
$$





